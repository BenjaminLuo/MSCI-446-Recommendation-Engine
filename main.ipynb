{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data, creating dataframe\n",
    "full_dataframe = pd.read_csv(\"amazon_co-ecommerce_sample.csv\")\n",
    "df = full_dataframe[['uniq_id', 'product_name', 'manufacturer', 'description', 'product_information', 'product_description', 'amazon_category_and_sub_category', 'customers_who_bought_this_item_also_bought', 'items_customers_buy_after_viewing_this_item']]\n",
    "# df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "* Dropping unneeded features\n",
    "* Removing empty data\n",
    "* Removing duplicates\n",
    "* Remove embedded special characters\n",
    "* Correct/Remove mispelt words\n",
    "* Remove common words\n",
    "* Tokenize by white space\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data:\n",
    "df = df.dropna(how='any',axis=0) \n",
    "df.reset_index()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop smaller categories with less than 20 items\n",
    "df['cleaned_category'] = df['amazon_category_and_sub_category'].map(lambda x: x.split(\">\", 1)[0])\n",
    "df = df.groupby(['cleaned_category']).filter(lambda x : len(x)>20)\n",
    "df.groupby(['cleaned_category']).count()['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating text columns for mining (note: assumes equal weights)\n",
    "df['details'] = df['product_name'] + \" \" + df['description'] + \" \" + df['product_description'] + \" \" + df['product_information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text mining: tokenize the key words\n",
    "\n",
    "from preprocessor import *\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df = resample(df, n_samples=100)  # Trimming data set because my CPU is dying\n",
    "\n",
    "df['cleaned_data'] = df['details'].map(lambda s: preprocess(s))\n",
    "# df['cleaned_data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finding the most frequent words\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# transformed_data_count = vectorizer.fit_transform(df['cleaned_data'])\n",
    "# temp = list(zip(vectorizer.get_feature_names_out(), np.ravel(transformed_data_count.sum(axis=0))))\n",
    "# sorted(temp, key=lambda x: x[1])[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the TF-IDF and appending it to the main dataframe\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "transformed_data = vectorizer.fit_transform(df['cleaned_data'])\n",
    "\n",
    "# Finding the most popular words according to TF-IDF\n",
    "temp = list(zip(vectorizer.get_feature_names_out(), np.ravel(transformed_data.sum(axis=0))))\n",
    "sorted(temp, key=lambda x: x[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IDF-TF into a dataframe where each feature is a word\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "df_transformed_data = pd.DataFrame(transformed_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_idf = df.join([df_transformed_data]).fillna(0)\n",
    "df_idf = df_idf.reset_index()\n",
    "df_idf.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning each row to a cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=14, random_state=1, max_iter=100, init='random')\n",
    "model = km.fit(transformed_data)\n",
    "labels = model.predict(transformed_data)\n",
    "\n",
    "df_idf['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the contents of each cluster\n",
    "clusters = {}\n",
    "n = 0\n",
    "for item in labels:\n",
    "    if item in clusters:\n",
    "        clusters[item].append(df_idf['product_name'][n])\n",
    "    else:\n",
    "        clusters[item] = [df_idf['product_name'][n]]\n",
    "    n += 1\n",
    "\n",
    "for item in clusters:\n",
    "    print(\"Cluster \", item)\n",
    "    for i in clusters[item]:\n",
    "        print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Compare against the actual recommendations from Amazon, is the data part of the same cluster? What is the manhattan distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling: Extracting product titles from the URLs of suggested items\n",
    "from crawler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_to_product_name(df['items_customers_buy_after_viewing_this_item'][0])\n",
    "\n",
    "# df = full_dataframe[['product_name', 'manufacturer', 'description', 'product_information', 'product_description', 'amazon_category_and_sub_category', 'customers_who_bought_this_item_also_bought', 'items_customers_buy_after_viewing_this_item']][:10]\n",
    "\n",
    "# Pre-Processing: Extracting the product names from the URL data\n",
    "# for i in range(1):\n",
    "#     if len(df['items_customers_buy_after_viewing_this_item'][i]) > 0:\n",
    "#         df['items_customers_buy_after_viewing_this_item'][i] = url_to_product_name(df['items_customers_buy_after_viewing_this_item'][i])\n",
    "\n",
    "# df['items_customers_buy_after_viewing_this_item']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
